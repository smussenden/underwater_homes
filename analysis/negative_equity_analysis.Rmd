---
title: "Negative Equity in the Property Market"
author: "Sean Mussenden | Howard Center for Investigative Journalism"
date: "11/6/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This memo describes initial data analysis done to identify U.S. communities where a significant portion of homes are "underwater", where the estimated value of a home is lower than the estimated principal balance of the mortgage (negative loan to value or LTV). The analysis also seeks to identify demographic and economic features of communities with high rates of negative LTV homes, and to understand how negative LTV rates have changed since 2009, when some communities had a majority of homes with negative LTVs. 

# Major findings

* Rates today are nowhere near where they were in 2009. 
* Some geographic concentration today. 
* 

# Load Packages and Settings
```{r}
# Turn off scientific notation
options(scipen=9999)

## Load packages
# For general data science goodness
library(tidyverse)

# For data cleaning
library(janitor)

# For working with datetime
library(lubridate)

# For reading in Excel files
library(readxl)

# For working with ZIP Codes
library(zipcode)

# For mapping
library(maps)
library(mapview)
library(sf)
library(leaflet)
library(leafpop)
library(leafem)
library(raster)
library(tigris)

# For pulling census data
library(tidycensus)

# For correlations
library(corrr)
library(moderndive)
library(Hmisc)

# For graphics
library(scales)
library(ggthemes)
library(DT)
library(ggpubr)

# Function to flatten correlation matrix
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}
```


## Load and clean negative equity data

Data on negative equity via CoreLogic.  Two data sets: negative equity share of all homes by county by month-year from 2009 to present; negative equity share of all homes by ZIP code for unknown month in 2019 (have a call in).  We are waiting for a third data set, which they sent to us, but had errors I identified (only had data for four states), of negative equity share of all homes by ZIP code by month-year from 2009-present. I've also reached out to Zillow for additional data.


```{r}

# Load negative equity share by county by month-year
underwater_county_month_year <- read_xlsx("../data/input_data/core_logic/negative_equity_share_county.xlsx")

# Clean negative equity share by county by month-year
underwater_county_month_year <- underwater_county_month_year %>%
  # create year column from yyyymm
  mutate(year=str_sub(yyyymm, 1,4)) %>%
  # create month column yyyymm
  mutate(month=str_sub(yyyymm,5,6)) %>%
  # select needed columns
  dplyr::select(state_code, fips_code, state_name, county_name,year, month, yyyymm, percent_negative_equity) %>%
  # fix busted fips codes by converting to character and adding a leading 0 to four digit codes
  mutate(fips_code = as.character(fips_code)) %>%
  mutate(fips_code = case_when(str_length(fips_code) < 5  ~ paste0("0",fips_code),
                                TRUE ~ fips_code)
         ) %>%
  mutate(percent_negative_equity = round(percent_negative_equity*100, 2)) 

# Create negative equity share by county by year, using average of 12 months in a given year as value for year.
underwater_county_year <- underwater_county_month_year %>%
  # Group by county and year
  group_by(state_code, fips_code, state_name, county_name, year) %>%
  # Average 12 months in each year
  summarise(percent_negative_equity = round(mean(percent_negative_equity),2)) %>%
  # Make the long data wide
  spread(year, percent_negative_equity) %>%
  # Fix column names
  rename_at(vars(matches("20")), funs(paste0("pct_negative_equity_y", .)))

# Remove underwater_county_month_year
rm(underwater_county_month_year)

# Load and clean ZIP Code from unknown time period in 2019
# Still need to get from CoreLogic an answer on time period

underwater_zips_2019 <- read_xlsx("../data/input_data/core_logic/corelogic_underwater_homes.xlsx") 

# Clean ZIP code in unknown time period in 2019
underwater_zips_2019 <- underwater_zips_2019 %>%
  # Fix column names
  clean_names() %>%
  # Fix zip codes
  mutate(zip_code = clean.zipcodes(zip_code)) %>%
  # Make negative equity share readable and standardize column name
  mutate(percent_negative_equity = round(share_of_homes_in_negative_equity*100, 2)) %>%
  dplyr::select(-share_of_homes_in_negative_equity)


```

## Load and clean U.S. Census data

Select demographic and economic variables via Tidycensus package.

``` {r}

# Load variables for exploration
# acs_variable <- load_variables(2017, "acs5", cache = TRUE)

# Define census api key
census_api_key("549950d36c22ff16455fe196bbbd01d63cfbe6cf")

# Get percent white, percent black, percent hispanic, poverty rate, income data by county

acs_county_white <- get_acs(geography = "county", variables = c("B02001_002"), geometry = FALSE, survey="acs5", year = 2017, summary_var ="B01001_001") %>%
  mutate(pct_white = estimate/summary_est) %>%
  dplyr::select(GEOID,NAME,pct_white) %>%
  clean_names()

acs_county_black <- get_acs(geography = "county", variables = c("B02001_003"), geometry = FALSE, survey="acs5", year = 2017, summary_var ="B01001_001") %>%
  mutate(pct_black = estimate/summary_est) %>%
  dplyr::select(GEOID,NAME,pct_black) %>%
  clean_names()

acs_county_hispanic <- get_acs(geography = "county", variables = c("B03001_003"), geometry = FALSE, survey="acs5", year = 2017, summary_var ="B01001_001") %>%
  mutate(pct_hispanic = estimate/summary_est) %>%
  dplyr::select(GEOID,NAME,pct_hispanic) %>%
  clean_names()

acs_county_poverty <- get_acs(geography = "county", variables = c("B06012_002"), geometry = FALSE, survey="acs5", year = 2017, summary_var ="B01001_001") %>%
  mutate(pct_poverty = estimate/summary_est) %>%
  dplyr::select(GEOID,NAME,pct_poverty) %>%
  clean_names()

acs_county_income <- get_acs(geography = "county", variables = c("B19013_001"), geometry = FALSE, survey="acs5", year = 2017) %>%
  dplyr::select(GEOID,NAME, median_household_income = estimate) %>%
  clean_names()

# Join the variables into a single data frame

acs_data_county <- acs_county_white %>%
  inner_join(acs_county_black) %>%
  inner_join(acs_county_hispanic) %>%
  inner_join(acs_county_poverty) %>%
  inner_join(acs_county_income) %>%
  mutate(pct_white = round(pct_white*100,2),
         pct_black = round(pct_black*100,2),
         pct_hispanic = round(pct_hispanic*100,2),
         pct_poverty = round(pct_poverty*100,2),
         )
  

# Get list of fipscodes from Tigris package
fips_codes <- fips_codes %>%
  mutate(fips_code = paste0(state_code,county_code)) 

# Join fips codes to table of census variables
acs_data_county <- acs_data_county %>%
  inner_join(fips_codes, by=c("geoid" = "fips_code"))

# Remove everything except for single acs_data_county table
rm(list=ls(pattern="acs_county"))

# Get percent white, percent black, percent hispanic, poverty rate, income data by ZCTA

acs_zcta_white <- get_acs(geography = "zcta", variables = c("B02001_002"), geometry = FALSE, survey="acs5", year = 2017, summary_var ="B01001_001") %>%
  mutate(pct_white = estimate/summary_est) %>%
  dplyr::select(GEOID,NAME,pct_white) %>%
  clean_names()

acs_zcta_black <- get_acs(geography = "zcta", variables = c("B02001_003"), geometry = FALSE, survey="acs5", year = 2017, summary_var ="B01001_001") %>%
  mutate(pct_black = estimate/summary_est) %>%
  dplyr::select(GEOID,NAME,pct_black) %>%
  clean_names()

acs_zcta_hispanic <- get_acs(geography = "zcta", variables = c("B03001_003"), geometry = FALSE, survey="acs5", year = 2017, summary_var ="B01001_001") %>%
  mutate(pct_hispanic = estimate/summary_est) %>%
  dplyr::select(GEOID,NAME,pct_hispanic) %>%
  clean_names()

acs_zcta_poverty <- get_acs(geography = "zcta", variables = c("B06012_002"), geometry = FALSE, survey="acs5", year = 2017, summary_var ="B01001_001") %>%
  mutate(pct_poverty = estimate/summary_est) %>%
  dplyr::select(GEOID,NAME,pct_poverty) %>%
  clean_names()

acs_zcta_income <- get_acs(geography = "zcta", variables = c("B19013_001"), geometry = FALSE, survey="acs5", year = 2017) %>%
  dplyr::select(GEOID,NAME, median_household_income = estimate) %>%
  clean_names()

# Join the variables into a single data frame

acs_data_zcta <- acs_zcta_white %>%
  inner_join(acs_zcta_black) %>%
  inner_join(acs_zcta_hispanic) %>%
  inner_join(acs_zcta_poverty) %>%
  inner_join(acs_zcta_income) %>%
  mutate(pct_white = round(pct_white*100,2),
         pct_black = round(pct_black*100,2),
         pct_hispanic = round(pct_hispanic*100,2),
         pct_poverty = round(pct_poverty*100,2),
         )
  

# Remove everything except for single acs_data_zcta table

rm(list=ls(pattern="acs_zcta|fips_codes"))

```

## Load and clean Zillow data

Loading select variables on health and status of real estate market by county and by ZIP code from Zillow.  

```{r}

# https://www.zillow.com/research/data/

# Read in Zillow Region ID to County Crosswalk
zillow_crosswalk <- read_csv("../data/input_data/zillow/CountyCrossWalk_Zillow.csv") %>%
  mutate(fips_code = as.character(FIPS)) %>%
  mutate(fips_code = case_when(str_length(fips_code) < 5  ~ paste0("0",fips_code),
                                TRUE ~ fips_code)
         ) %>%
  dplyr::select(CountyRegionID_Zillow, fips_code) 

# Home values summary for current month (Oct 2019) and historical comparison by county 

county_ZHVI_summary_current_month <- read_csv("../data/input_data/zillow/County_Zhvi_Summary_AllHomes.csv") %>%
  mutate(five_year_pct_change = round(`5Year`*100,2),
         ten_year_pct_change = round(`10Year`*100,2),
         month_pct_change = round(MoM*100,2),
         quarter_pct_change = round(QoQ*100,2),
         year_pct_change = round(YoY*100,2)) %>%
  inner_join(zillow_crosswalk, by=c("RegionID" = "CountyRegionID_Zillow")) %>%
  dplyr::select(fips_code, RegionName, State, Zhvi,month_pct_change, quarter_pct_change, year_pct_change, five_year_pct_change, ten_year_pct_change)


# Home values summary for current month (Oct 2019) and historical comparison by ZIP

Zip_ZHVI_summary_current_month <- read_csv("../data/input_data/zillow/Zip_Zhvi_Summary_AllHomes.csv") %>%
   mutate(five_year_pct_change = round(`5Year`*100,2),
         ten_year_pct_change = round(`10Year`*100,2),
         month_pct_change = round(MoM*100,2),
         quarter_pct_change = round(QoQ*100,2),
         year_pct_change = round(YoY*100,2)) %>%
  dplyr::select(zip_code = RegionName, State, Zhvi,month_pct_change, quarter_pct_change, year_pct_change, five_year_pct_change, ten_year_pct_change)

# Year over year ZHVI forecast current month (Oct 2019), all geographics (long sheet, sted wide)
all_regions_forecasts <- read_csv("../data/input_data/zillow/AllRegionsForePublic-1.csv")

# Create year over year ZHVI forecast just for counties
county_forecasts <- all_regions_forecasts %>%
  filter(Region == "County")

# Create year over year ZHVI forecast just for ZIP
zip_forecasts <- all_regions_forecasts %>%
  filter(Region=="Zip")

# Join county summary plus forecast
county_summary_forecast <- county_ZHVI_summary_current_month %>%
  inner_join(county_forecasts, by=c("State" = "StateName","RegionName" = "CountyName")) %>%
  mutate(forecast_year_pct_change = round(ForecastYoYPctChange,2)) %>%
  dplyr::select(-Region, -RegionName.y, -CityName, -ForecastYoYPctChange)


# Join ZIP summary plus forecast
zip_summary_forecast <- Zip_ZHVI_summary_current_month %>%
  inner_join(zip_forecasts, by=c("zip_code" = "RegionName")) %>%
  mutate(forecast_year_pct_change = round(ForecastYoYPctChange,2)) %>%
  dplyr::select(-Region, -StateName, -CountyName, -CityName, -ForecastYoYPctChange) 

# For later analysis Time series home values by County and Zip
#county_ZHVI_time_series <- read_csv("../data/input_data/zillow/County_Zhvi_AllHomes.csv")
#Zip_ZHVI_time_series <- read_csv("../data/input_data/zillow/Zip_Zhvi_AllHomes.csv")

# Remove unneeded files
rm(list=c("county_ZHVI_summary_current_month","Zip_ZHVI_summary_current_month", "all_region_forecasts","zip_forecasts", "county_forecasts", "zillow_crosswalk"))

```

# Load and clean misc county data

```{r}

# County rural urban code designation from USDA
# Spectrum from 1 (most urban) to 9 (most rural)
rural_urban <- read_xls("../data/input_data/rural_urban_codes/ruralurbancodes2013.xls") %>%
  rename(fips_code = FIPS)

# Mortgage delinquicies
# Percentage of deliquent mortgages by year on average
# https://www.consumerfinance.gov/data-research/mortgage-performance-trends/mortgages-30-89-days-delinquent/
delinquicies <- read_csv("../data/input_data/mortgages/CountyMortgagesPercent-30-89DaysLate-thru-2019-03.csv") %>%
  filter(RegionType != "National") %>%
  pivot_longer(cols= "2008-01":"2019-03", names_to="year_month", values_to="pct_late") %>%
  mutate(year=str_sub(year_month, start=1L, end=4L)) %>%
  group_by(State, Name, FIPSCode, year) %>%
  summarise(pct_late = round(mean(pct_late),2)) %>%
  ungroup() %>%
  mutate(FIPSCode = str_remove(FIPSCode,"'")) %>%
  mutate(FIPSCode = str_remove(FIPSCode,"'")) %>%
  pivot_wider(names_from = year, values_from=pct_late, names_prefix = "pct_late_y")

# BLS unemployment data, seasonally adjusted 2018 average
# https://www.bls.gov/lau/
# Labor force data by county, 2018 annual averages (TXT, XLS)

unemployment <- read_xlsx("../data/input_data/bls/laucnty18.xlsx") %>%
  mutate(fips_code = paste0(state_fips,county_fips)) %>%
  dplyr::select(fips_code, county_state, unemployment_rate_2018 = unemployment_rate)
  
  
```

# Join together everything

```{r}
## County data frames

# 2446 counties
county_uw_ts_census_rural <- underwater_county_year %>%
  ungroup() %>%
  inner_join(acs_data_county, by=c("fips_code" = "geoid")) %>%
  dplyr::select(fips_code, state_name = state_name.x, county_name, starts_with("pct")) %>%
  inner_join(rural_urban) %>%
  dplyr::select(-State,-Population_2010, -County_Name, -Description) %>%
  inner_join(unemployment) %>%
  dplyr::select(-county_state)


# 1477 counties
county_uw_2019_census_rural_zillow <- underwater_county_year %>%
  ungroup() %>%
  dplyr::select(state_code:county_name, pct_negative_equity_y2009, pct_negative_equity_y2019) %>%
  inner_join(acs_data_county, by=c("fips_code" = "geoid")) %>%
  dplyr::select(fips_code, state_name = state_name.x, county_name, starts_with("pct")) %>%
  inner_join(rural_urban) %>%
  dplyr::select(-State,-Population_2010, -County_Name, -Description) %>%
  inner_join(county_summary_forecast) %>%
  dplyr::select(-RegionName, -State) %>%
  inner_join(unemployment) %>%
  dplyr::select(-county_state)

# 444 counties
county_uw_2019_census_rural_zillow_dq <- underwater_county_year %>%
  ungroup() %>%
  dplyr::select(state_code:county_name, pct_negative_equity_y2009, pct_negative_equity_y2019) %>%
  inner_join(acs_data_county, by=c("fips_code" = "geoid")) %>%
  dplyr::select(fips_code, state_name = state_name.x, county_name, starts_with("pct")) %>%
  inner_join(rural_urban) %>%
  dplyr::select(-State,-Population_2010, -County_Name, -Description) %>%
  inner_join(county_summary_forecast) %>%
  dplyr::select(-RegionName, -State) %>%
  inner_join(delinquicies, by=c("fips_code" = "FIPSCode")) %>%
  dplyr::select(-State, -Name, -starts_with("pct_late"), pct_late_y2009, pct_late_y2019) %>%
  inner_join(unemployment) %>%
  dplyr::select(-county_state)

## ZCTA dataframes

zcta_uw_census <- underwater_zips_2019 %>%
  ungroup() %>%
  inner_join(acs_data_zcta, by=c("zip_code" = "geoid")) %>%
  dplyr::select(zip_code, everything(),-name) %>%
  inner_join(zip_summary_forecast) %>%
  dplyr::select(-State) 

rm(list=setdiff(ls(), c("county_uw_ts_census_rural", "county_uw_2019_census_rural_zillow", "county_uw_2019_census_rural_zillow_dq", "zcta_uw_census", "flattenCorrMatrix")))

```


## Load and clean zillow data from data.world
Strategy
What are the ramifications of negative equity?

Negative equity can cast a shadow over the economy, effectively prolonging an economic downturn. Just as skyrocketing home values can drive confidence and spending, negative equity can keep people home, literally. Many people who might have found jobs in other markets were tied to their homes and unable to sell because the money they would make from selling would not be enough to pay off their mortgages. Negative equity can have a number of other chilling impacts on local housing markets, disproportionately impacting minority communities and owners of lower-valued homes, exacerbating inventory shortages and increasing the likelihood of foreclosure for underwater homeowners.
# https://www.zillow.com/research/negative-equity-race-q3-2016-14063/
# https://www.zillow.com/research/q3-2016-negative-equity-report-13954/


https://www.zillow.com/research/q3-2016-negative-equity-report-13954/
But this drop, while encouraging, masks the often very wide divide that remains between the top of the market and the bottom of the market. In a number of large markets, the spread between the negative equity rate at the top and the bottom of the market is alarmingly wide. In Detroit, for example, the negative equity rate among top-tier homes is 4.4 percent; among bottom-tier homes, the negative equity rate is almost ten times higher at 39 percent – a gap of 34.6 percentage points, the largest gap among the 35 largest metros analyzed. The gap between top-tier and bottom-tier negative equity is more than 20 percentage points in an additional four large metros: Cleveland (28.1 points), St. Louis (22 points), Atlanta (21 points) and Chicago (20.5 points).



```{r}
# https://datadotworld.github.io/data.world-r/r-rstudio.html
# install.packages('data.world')
library(data.world)

# Store token
saved_cfg <- data.world::save_config("eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJyLWFuZC1yLXN0dWRpbzpzbXVzc2VuZGVuIiwiaXNzIjoiY2xpZW50OnItYW5kLXItc3R1ZGlvOmFnZW50OnNtdXNzZW5kZW46OmE1YWFmYjRhLTM4NTItNDc4Mi05Y2JjLTgzMDVmN2UyZWE5MCIsImlhdCI6MTU3NDQ1NzczMCwicm9sZSI6WyJ1c2VyX2FwaV9hZG1pbiIsInVzZXJfYXBpX3JlYWQiLCJ1c2VyX2FwaV93cml0ZSJdLCJnZW5lcmFsLXB1cnBvc2UiOnRydWUsInNhbWwiOnt9fQ.9dc8h4PyGmm3CQcEgoG8fTFsA-Qz8nYNkCd39CiWRrUQV5slhWNBGra82yFQhQuwCfvdfZlL06utF1zaJxoNlw")
data.world::set_config(saved_cfg)

# Define path to dataset
negative_equity_summary_url <- "https://data.world/zillow-data/negative-equity-summary"
# Define query to load dataset
negative_equity_summary_query <- data.world::qry_sql("SELECT * FROM NESummary_2017Q1_Public")
# Get data
negative_equity_summary <- data.world::query(negative_equity_summary_query, dataset = negative_equity_summary_url)
# Filter by county
negative_equity_summary_county <- negative_equity_summary %>%
  filter(regiontype == "County")
# Filter by zip
negative_equity_summary_zip <- negative_equity_summary %>%
  filter(regiontype == "Zip")

# Join to county data  

x <- negative_equity_summary_county %>%
  inner_join(zillow_crosswalk, by=c("regionid" = "CountyRegionID_Zillow")) %>%
  dplyr::select(fips_code, everything()) %>%
  inner_join(acs_data_county, by=c("fips_code" = "geoid")) %>%
  mutate(majority_minority = case_when(pct_white < 50 ~ TRUE,
                                       TRUE ~ FALSE))

# Averages

y <- x %>%
  group_by(majority_minority) %>%
  summarise_if(is.numeric, mean, na.rm=TRUE)

# Correlate

x %>%
  select_if(is.numeric) %>%
  correlate() %>%
  dplyr::select(rowname, pct_white, pct_black, pct_hispanic, median_household_income) %>%
  mutate(abs_pct_white=abs(pct_white)) %>%
  arrange(desc(abs_pct_white)) %>%
  dplyr::select(-abs_pct_white)
select_i


https://data.world/zillow-data/negative-equity-tiers
https://data.world/zillow-data/negative-equity-summary
https://data.world/zillow-data/negative-equity-time-series
https://data.world/zillow-data/negative-equity-summary

```

## Load Shapefiles

```{r}
# ZIP Code Points
data(zipcode)

options(tigris_use_cache=TRUE)

# ZCTA shapefiles
zctas <- zctas(cb=TRUE)

# Counties
counties <- counties(cb = TRUE)

```

## Explaining the differences
In general, amongst the highest negative equity zips, there's a general trend.  The higher the rate of af american and hispanics, the higher the rate.

```{r}

# Remove non-numerical columns, and eliminate less significant negative equity zips
zcta_uw_census_trimmed <- zcta_uw_census %>%
  dplyr::select(-zip_code,-state_name,-county_name) %>%
  filter(percent_negative_equity >=5) %>%
  mutate(log_median_household_income = log(median_household_income))

# Create correlation matrix
zcta_uw_census_correlation_matrix <- rcorr(as.matrix(zcta_uw_census_trimmed))

# Flatten correlation table, to only include significant predictors p < .05
zcta_uw_census_correlation_matrix <- flattenCorrMatrix(zcta_uw_census_correlation_matrix$r, zcta_uw_census_correlation_matrix$P) %>%
  mutate(absolute_cor = abs(cor)) %>%
  filter(row == "percent_negative_equity") %>%
  filter(p < .05) %>%
  arrange(desc(absolute_cor)) %>%
  dplyr::select(-absolute_cor)

# Print correlation matrix.  Let's us see both r (correlation coefficient) and p value for each predictor on an individual level
zcta_uw_census_correlation_matrix %>%
  datatable()

# Create multiple linear model to explain (creating two, one with all variables and one with selected variables)

# All variables
zcta_uw_census_linear_model_all <- lm(percent_negative_equity ~ ., data = zcta_uw_census_trimmed)

# Print a summary of the model with all variables
summary(zcta_uw_census_linear_model_all)

# Print a regression table and summary of all variables
get_regression_table(zcta_uw_census_linear_model_all) %>% 
  datatable()
get_regression_summaries(zcta_uw_census_linear_model_all) %>%
  mutate(r = sqrt(r_squared)) %>% 
  dplyr::select(r_squared, adj_r_squared, r, everything()) %>%
  datatable() 

# Select variables
zcta_uw_census_linear_model_select <- lm(percent_negative_equity ~ pct_white+pct_hispanic+five_year_pct_change+ten_year_pct_change, data = zcta_uw_census_trimmed)

# Print a summary of the model with select variables
summary(zcta_uw_census_linear_model_select)

# Print a regression table of select variables
get_regression_table(zcta_uw_census_linear_model_select) %>% 
  datatable()
get_regression_summaries(zcta_uw_census_linear_model_select) %>%
  mutate(r = sqrt(r_squared)) %>% 
  dplyr::select(r_squared, adj_r_squared, r, everything()) %>%
  datatable() 

# Plot one property using ggpubr scatter package

# Just as a reminder, here's what a perfect correlation plot looks like.  Tightly grouped around the prediction line of best fit.
ggscatter(zcta_uw_census_trimmed, x="percent_negative_equity", "percent_negative_equity", add="reg.line", add.params = list(color = "blue", fill = "lightgray"), conf.int=TRUE) +
  stat_cor(method = "pearson", label.x = 3, label.y = 15)

# Make facet graphs
graph_data <- zcta_uw_census_trimmed %>%
  gather(-percent_negative_equity, key = "var", value = "value") %>%
  inner_join(zcta_uw_census_correlation_matrix, by=c("var" = "column")) %>%
  dplyr::select(-row, -cor, -p) %>%
  ggscatter(x="value", y="percent_negative_equity", add="loess", add.params = list(color = "blue", fill = "lightgray"), conf.int=TRUE, mean.point = TRUE) +
  stat_cor(method = "pearson", label.x = 0, label.y = 25)

facet(graph_data, facet.by="var", scales="free_x", ncol=4)

```

```{r}

# Data.world in R -- lets build a model for q12017 that explains everything.  Use it to update for now. 


# https://data.world/zillow-data
# Let's build a model using this rate https://data.world/zillow-data/negative-equity-summary
# https://data.world/zillow-data/negative-equity-tiers
# https://www.census.gov/content/dam/Census/programs-surveys/ahs/publications/Drowning_in_Debt.pdf
# Owner to renter
# https://data.world/zillow-data/negative-equity-time-series

# Remove non-numerical columns, and eliminate less significant negative equity zips
county_uw_ts_census_rural_trimmed <- county_uw_ts_census_rural %>%
  dplyr::select(-fips_code,-state_name,-county_name, -pct_negative_equity_y2010, -pct_negative_equity_y2011, -pct_negative_equity_y2012, -pct_negative_equity_y2013, -pct_negative_equity_y2014, -pct_negative_equity_y2015, -pct_negative_equity_y2016, -pct_negative_equity_y2017, -pct_negative_equity_y2018) 
  # %>% filter(pct_negative_equity_y2019 >=5) 

# Create correlation matrix
county_uw_ts_census_rural_correlation_matrix <- rcorr(as.matrix(county_uw_ts_census_rural_trimmed))

# Flatten correlation table, to only include significant predictors p < .05
county_uw_ts_census_rural_correlation_matrix <- flattenCorrMatrix(county_uw_ts_census_rural_correlation_matrix$r, county_uw_ts_census_rural_correlation_matrix$P) %>%
  mutate(absolute_cor = abs(cor)) %>%
  filter(row == "pct_negative_equity_y2019") %>%
  filter(p < .05) %>%
  arrange(desc(absolute_cor)) %>%
  dplyr::select(-absolute_cor)

# Print correlation matrix.  Let's us see both r (correlation coefficient) and p value for each predictor on an individual level
county_uw_ts_census_rural_correlation_matrix %>%
  datatable()

# Create multiple linear model to explain (creating two, one with all variables and one with selected variables)

# All variables
county_uw_ts_census_rural_linear_model_all <- lm(pct_negative_equity_y2019 ~ ., data = county_uw_ts_census_rural_trimmed)

# Print a summary of the model with all variables
summary(county_uw_ts_census_rural_linear_model_all)

# Print a regression table and summary of all variables
get_regression_table(county_uw_ts_census_rural_linear_model_all) %>% 
  datatable()

get_regression_summaries(county_uw_ts_census_rural_linear_model_all) %>%
  mutate(r = sqrt(r_squared)) %>% 
  dplyr::select(r_squared, adj_r_squared, r, everything()) %>%
  datatable() 

# Select variables
county_uw_ts_census_rural_linear_model_select <- lm(pct_negative_equity_y2019 ~ pct_white+pct_negative_equity_y2009+pct_poverty+pct_black+pct_hispanic, data = county_uw_ts_census_rural_trimmed)

# Print a summary of the model with select variables
summary(county_uw_ts_census_rural_linear_model_select)

# Print a regression table of select variables
get_regression_table(county_uw_ts_census_rural_linear_model_select) %>% 
  datatable()

get_regression_summaries(county_uw_ts_census_rural_linear_model_select) %>%
  mutate(r = sqrt(r_squared)) %>% 
  dplyr::select(r_squared, adj_r_squared, r, everything()) %>%
  datatable() 

# Plot one property using ggpubr scatter package

# Just as a reminder, here's what a perfect correlation plot looks like.  Tightly grouped around the prediction line of best fit.
ggscatter(zcta_uw_census_trimmed, x="percent_negative_equity", "percent_negative_equity", add="reg.line", add.params = list(color = "blue", fill = "lightgray"), conf.int=TRUE) +
  stat_cor(method = "pearson", label.x = 3, label.y = 15)

# Make facet graphs
graph_data <- zcta_uw_census_trimmed %>%
  gather(-percent_negative_equity, key = "var", value = "value") %>%
  inner_join(zcta_uw_census_correlation_matrix, by=c("var" = "column")) %>%
  dplyr::select(-row, -cor, -p) %>%
  ggscatter(x="value", y="percent_negative_equity", add="reg.line", add.params = list(color = "blue", fill = "lightgray"), conf.int=TRUE, mean.point = TRUE) +
  stat_cor(method = "pearson", label.x = 0, label.y = 25)

facet(graph_data, facet.by="var", scales="free_x", ncol=3)
```

```{r}

```



```{r}
county_uw_ts_census_rural %>%
  dplyr::select(-fips_code,-state_name,-county_name) %>%
  #filter(pct_negative_equity_y2019 >=1) %>%
  correlate() %>%
  dplyr::select(rowname, pct_negative_equity_y2019) %>%
  arrange(desc(pct_negative_equity_y2019))




library("Hmisc")

x <- county_uw_ts_census_rural %>%
  dplyr::select(-fips_code,-state_name,-county_name) %>%
  filter(pct_negative_equity_y2019 >=1) 

#res2 <- rcorr(as.matrix(x))
#res2
#y <- as_tibble(res2$r)
#y
#res2$P
res2 <- rcorr(as.matrix(x))

flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}

y <- flattenCorrMatrix(res2$r, res2$P) %>%
  mutate(absolute_cor = abs(cor)) %>%
  filter(row == "percent_negative_equity") %>%
  filter(p < .05) %>%
  arrange(desc(absolute_cor)) %>%
  dplyr::select(-absolute_cor)

y

score_model <- lm(pct_negative_equity_y2019 ~ ., data = x)
summary(score_model)

#score_model

get_regression_table(score_model) %>% datatable()
get_regression_summaries(score_model) %>% datatable()


```
R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.

The definition of R-squared is fairly straight-forward; it is the percentage of the response variable variation that is explained by a linear model. Or:

R-squared = Explained variation / Total variation

R-squared is always between 0 and 100%:

    0% indicates that the model explains none of the variability of the response data around its mean.
    100% indicates that the model explains all the variability of the response data around its mean.

In general, the higher the R-squared, the better the model fits your data. However, there are important conditions for this guideline that I’ll talk about both in this post and my next post.
```{r}
county_uw_2019_census_rural_zillow %>%
  dplyr::select(-fips_code,-state_name,-county_name) %>%
  #filter(pct_negative_equity_y2019 >=1) %>%
  correlate() %>%
  dplyr::select(rowname, pct_negative_equity_y2019) %>%
  arrange(desc(pct_negative_equity_y2019))



library("Hmisc")

x <- county_uw_2019_census_rural_zillow %>%
  dplyr::select(-fips_code,-state_name,-county_name) #%>%
 # filter(pct_negative_equity_y2019 >=1) 

#res2 <- rcorr(as.matrix(x))
#res2
#y <- as_tibble(res2$r)
#y
#res2$P
res2 <- rcorr(as.matrix(x))

flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}

y <- flattenCorrMatrix(res2$r, res2$P) %>%
  mutate(absolute_cor = abs(cor)) %>%
  filter(row == "percent_negative_equity") %>%
  filter(p < .05) %>%
  arrange(desc(absolute_cor)) %>%
  dplyr::select(-absolute_cor)

y

score_model <- lm(pct_negative_equity_y2019 ~ ., data = x)
summary(score_model)

#score_model

get_regression_table(score_model) %>% datatable()
get_regression_summaries(score_model) %>% datatable()

```

```{r}
county_uw_2019_census_rural_zillow_dq %>%
  dplyr::select(-fips_code,-state_name,-county_name) %>%
  #filter(pct_negative_equity_y2019 >=1) %>%
  correlate() %>%
  dplyr::select(rowname, pct_negative_equity_y2019) %>%
  arrange(desc(pct_negative_equity_y2019))

library("Hmisc")

x <- county_uw_2019_census_rural_zillow_dq %>%
  dplyr::select(-fips_code,-state_name,-county_name) #%>%
 # filter(pct_negative_equity_y2019 >=1) 

#res2 <- rcorr(as.matrix(x))
#res2
#y <- as_tibble(res2$r)
#y
#res2$P
res2 <- rcorr(as.matrix(x))

flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}

y <- flattenCorrMatrix(res2$r, res2$P) %>%
  mutate(absolute_cor = abs(cor)) %>%
  filter(row == "percent_negative_equity") %>%
  filter(p < .05) %>%
  arrange(desc(absolute_cor)) %>%
  dplyr::select(-absolute_cor)

y

score_model <- lm(pct_negative_equity_y2019 ~ ., data = x)
summary(score_model)

#score_model

get_regression_table(score_model) %>% datatable()
get_regression_summaries(score_model) %>% datatable()


```

```{r}

### Need to add in home price appreciation and mortgage origination as percentage of application
# Pull in NYT classification of urban rural

underwater_zips_2019_join <- underwater_zips_2019 %>%
  inner_join(acs_data_zcta, by=c("zip_code" = "geoid")) %>%
  inner_join(Zip_ZHVI_summary_current_month, by=c("zip_code" = "RegionName")) %>% 
  #%>%
  #filter(!is.na(percent_negative_equity),
   #      pct_white != "NaN",
   #      !is.na(median_household_income)) %>%
  filter(percent_negative_equity >=5) 
 
corr_table_zcta <- underwater_zips_2019_join %>%
  dplyr::select(-matches("code|name|Date|Region|State|Metro|County|City|Month|Quarter|Last|Time")) %>%
  dplyr::select(matches("percent|pct|year")) %>%
  correlate() %>%
  dplyr::select(rowname, percent_negative_equity)

print(corr_table_zcta)
#https://www.dataquest.io/blog/statistical-learning-for-predictive-modeling-r/
#install.packages("moderndive")

score_model <- lm(percent_negative_equity ~ pct_white+pct_black+pct_hispanic+PctFallFromPeak, data = underwater_zips_2019_join)
get_regression_table(score_model) %>% datatable()
get_regression_summaries(score_model) %>% datatable()

ggplot(underwater_zips_2019_join) +
  geom_point(aes(percent_negative_equity, pct_white)) +
  labs(x="Percent Negative Equity", y="Percent White", title="", caption = "") +
  geom_smooth(aes(percent_negative_equity, pct_white), method = "lm", se = FALSE) 

ggplot(underwater_zips_2019_join) +
  geom_point(aes(percent_negative_equity, pct_black)) +
  labs(x="Percent Negative Equity", y="Percent Black", title="", caption = "") +
  geom_smooth(aes(percent_negative_equity, pct_black), method = "lm", se = FALSE) 

ggplot(underwater_zips_2019_join) +
  geom_point(aes(percent_negative_equity, pct_hispanic)) +
  labs(x="Percent Negative Equity", y="Percent Hispanic", title="", caption = "") +
  geom_smooth(aes(percent_negative_equity, pct_hispanic), method = "lm", se = FALSE) 



```

# Analysis

In the average and median U.S. county, underwater rates nowhere near as high as they used to be in the years immediately following the crash of the housing market. In 2009, the average U.S. county had 5.5 percent of homes with negative equity, compared to 1.52 percent today.  The median showed a less dramatic decline. The 2009 mean is skewed up by some extremely high rates in select counties in 2009.  

```{r}
underwater_county_year %>%
  pivot_longer(cols=c("y2009", "y2010", "y2011", "y2012","y2013","y2014","y2015","y2016","y2017","y2018","y2019"), names_to = "year", values_to = "percent_negative_equity") %>%
  group_by(year) %>%
  filter(!is.na(percent_negative_equity)) %>%
  summarise(median_percent_negative_equity = round(median(percent_negative_equity),2),
            mean_percent_negative_equity = round(mean(percent_negative_equity),2)
            )
```

```{r}
underwater_county_year %>%
  filter(y2009 > 0) %>%
  pivot_longer(cols=c("y2009", "y2010", "y2011", "y2012","y2013","y2014","y2015","y2016","y2017","y2018","y2019"), names_to = "year", values_to = "percent_negative_equity") %>%
  group_by(year) %>%
  filter(!is.na(percent_negative_equity)) %>%
  summarise(median_percent_negative_equity = round(median(percent_negative_equity),2),
            mean_percent_negative_equity = round(mean(percent_negative_equity),2)
            )
```

The distribution of counties shows this.  In 2009, there were lots of counties with negative equity rates above 20 percent.  In 2019, only a handful have rates higher than 10.

```{r}

underwater_county_year %>%
  pivot_longer(cols=c("y2009", "y2010", "y2011", "y2012","y2013","y2014","y2015","y2016","y2017","y2018","y2019"), names_to = "year", values_to = "percent_negative_equity") %>%
  filter(str_detect(year,"y2009|y2012|y2015|y2019")) %>%
  filter(percent_negative_equity > 0) %>%
  ggplot(aes(percent_negative_equity)) +
  geom_histogram(binwidth = 5) +
  facet_wrap(~year)


#underwater_county_year %>%
#  pivot_longer(cols=c("y2009", "y2010", "y2011", "y2012","y2013","y2014","y2015","y2016","y2017","y2018","y2019"), names_to = "year", values_to = #"percent_negative_equity") %>%
#  group_by(year) %>%
#  filter(!is.na(percent_negative_equity)) %>%
#  summarise(median_percent_negative_equity = round(median(percent_negative_equity),2),
 #           mean_percent_negative_equity = round(mean(percent_negative_equity),2)
 #           )
```

## Negative Equity Clusters in 2019 

Though negative equity rates are nowhere near historical levels, the analysis identified 10 clusters with a higher negative equity rates. 

In the map below, click on the two-letter buttons at right to zoom to that cluster. 

Click on each ZIP code to see info about each county.

Clusters
* MD1 | Maryland+DC | Cluster of ZIP Codes in Prince George's County and Anacostia in D.C. and down through Waldorm, in Charles County.
* MD2 | Maryland Eastern Shore | Huge chunks of the lower Eastern Shore, through Salisbury and Ocean City.
* NJ1 | South New Jersey | Large swaths of southern New Jersey, including Atlantic City, Philly suburbs.
* NJ2 | North Jersey+NYC | New York City suburbs, including Newark, Elizabeth, Paterson, parts of Queens
* CT | Connecticut | Several areas, including Hartford and Waterbury, Bridgeport and New Haven, up to Rhode Island border.
* IA | Iowa | a bunch of communities in Iowa with no real clusters. Can't make heads or tails of this.
* FL | Florida | Worst issues in Miami, Hialeah and Homestead, scatterd parts of the state.
* IL | Chicago | Huge issues surrounding Chicago, especially on South Side
* CA | California | Issues in Fresno, south of Monterey, scattered throughout. 
* NV | Las Vegas | 
* No buttons, but interesting | Baton Rouge, New Orleans, Atlanta, North Dakota


```{r}

# Filter for only high negative equity zip codes

underwater_zips_2019_x <- underwater_zips_2019 %>%
  filter(percent_negative_equity >= 5)

# Join zip code coordinates to negative equity by zipcode
underwater_zctas_2019 <- geo_join(zctas, underwater_zips_2019_x, 'GEOID10', 'zip_code', 
how = "inner")

# Color Scheme
binpal <- colorBin("plasma", underwater_zctas_2019$percent_negative_equity, 5, pretty = FALSE)

# Draw map
leaflet(underwater_zctas_2019) %>%
   addProviderTiles(providers$CartoDB.Positron) %>%
   #addProviderTiles(providers$Wikimedia) %>%
   addPolygons(fillColor = ~binpal(percent_negative_equity), weight = 1, smoothFactor = 0.5, opacity = 0.1, fillOpacity = 0.5, color="black", popup = popupTable(underwater_zctas_2019)) %>%
   setView(-95, 39.335359608681216, 4) %>%
   addHomeButton(extent(-98.39355468750001, -89.60449218750001, 39.85072092501597, 43.8899753738369), "IA") %>%
   addHomeButton(extent(-115.44021606445314, -114.89089965820314, 36.04521273039952, 36.318998009207924), "NV") %>%
   addHomeButton(extent(-127.24365234375001, -109.66552734375001, 31.98944183792288, 40.713955826286046), "CA") %>%
   addHomeButton(extent(-88.33145141601564, -87.23281860351564, 41.466399253078876, 41.97276436226528), "IL") %>%
   addHomeButton(extent(-80.82092285156251, -79.72229003906251, 25.517657429994035, 26.12831612064242), "FL") %>%
   addHomeButton(extent(-73.95446777343751, -71.75720214843751, 41.03793062246529, 42.05337156043361), "CT") %>%
   addHomeButton(extent(-74.46807861328126, -73.91876220703126, 40.57484790030712, 40.83199550584334), "NJ2") %>%
   addHomeButton(extent(-75.86883544921876, -73.67156982421876, 39.16414104768742, 40.20824570152502), "NJ1") %>%
   addHomeButton(extent(-76.95098876953126, -74.75372314453126, 38.10214399750345, 39.16201148082406), "MD2") %>%
   addHomeButton(extent(-77.1947479248047, -76.6454315185547, 38.791021386961596, 39.0549177529185), "MD1") %>%
   addHomeButton(extent(-131.74804687500003, -61.43554687500001, 18.812717856407776, 52.908902047770255 ), "U.S.") %>%
   addLogo("https://jeroenooms.github.io/images/banana.gif",
        position = "bottomleft",
        offset.x = 5,
        offset.y = 40,
        width = 100,
        height = 100) %>%
   addLegend("bottomleft", 
             pal = colorBin("plasma", underwater_zctas_2019$percent_negative_equity), 
             values = underwater_zctas_2019$percent_negative_equity,
    title = "% Homes Negative Equity 2019",
    labFormat = labelFormat(prefix = ""),
    opacity = 1
  ) %>%
  addEasyButton(easyButton(
    states = list(
      easyButtonState(
        stateName="unfrozen-markers",
        icon="ion-toggle",
        title="Get Bounding box",
        onClick = JS("
                     function(btn, map) {
                        alert(
                         map.getBounds().getWest() + ', ' + map.getBounds().getEast() + ', ' + map.getBounds().getSouth() + ', ' + map.getBounds().getNorth() 
                        );
                        
                     }")
      )
    )
  )
)
```

## Map current yearly and historical average county negative equity rate

We don't have time series data by ZIP Code, but we do have it by county.  Some interesting trends between 2009 and 2019, by comparing two maps.

The first map shows negative equity rates by county in 2019, and the second in 2009.

```{r}

# Filter for 2019
underwater_county_year_2019 <- underwater_county_year %>%
  dplyr::select(state_code, fips_code, state_name, county_name, y2019) %>%
  filter(y2019 >= 4)

# Join zip code coordinates to negative equity by zipcode
underwater_county_year_2019 <- geo_join(counties, underwater_county_year_2019, 'GEOID', 'fips_code', 
how = "inner")  

# Color Scheme
binpal <- colorBin("plasma", underwater_county_year_2019$y2019, 5, pretty = FALSE)

# Draw map
leaflet(underwater_county_year_2019) %>%
   addProviderTiles(providers$CartoDB.Positron) %>%
   #addProviderTiles(providers$Wikimedia) %>%
   addPolygons(fillColor = ~binpal(y2019), weight = 1, smoothFactor = 0.5, opacity = 0.1, fillOpacity = 0.5, color="black", popup = popupTable(underwater_county_year_2019)) %>%
   setView(-95, 39.335359608681216, 4) %>%
   addHomeButton(extent(-98.39355468750001, -89.60449218750001, 39.85072092501597, 43.8899753738369), "IA") %>%
   addHomeButton(extent(-115.44021606445314, -114.89089965820314, 36.04521273039952, 36.318998009207924), "NV") %>%
   addHomeButton(extent(-127.24365234375001, -109.66552734375001, 31.98944183792288, 40.713955826286046), "CA") %>%
   addHomeButton(extent(-88.33145141601564, -87.23281860351564, 41.466399253078876, 41.97276436226528), "IL") %>%
   addHomeButton(extent(-80.82092285156251, -79.72229003906251, 25.517657429994035, 26.12831612064242), "FL") %>%
   addHomeButton(extent(-73.95446777343751, -71.75720214843751, 41.03793062246529, 42.05337156043361), "CT") %>%
   addHomeButton(extent(-74.46807861328126, -73.91876220703126, 40.57484790030712, 40.83199550584334), "NJ2") %>%
   addHomeButton(extent(-75.86883544921876, -73.67156982421876, 39.16414104768742, 40.20824570152502), "NJ1") %>%
   addHomeButton(extent(-76.95098876953126, -74.75372314453126, 38.10214399750345, 39.16201148082406), "MD2") %>%
   addHomeButton(extent(-77.1947479248047, -76.6454315185547, 38.791021386961596, 39.0549177529185), "MD1") %>%
   addHomeButton(extent(-131.74804687500003, -61.43554687500001, 18.812717856407776, 52.908902047770255 ), "U.S.") %>%
   addLogo("https://jeroenooms.github.io/images/banana.gif",
        position = "bottomleft",
        offset.x = 5,
        offset.y = 40,
        width = 100,
        height = 100) %>%
   addLegend("bottomleft", 
             pal = binpal, 
             values = underwater_county_year_2019$y2019,
    title = "% Homes Negative Equity 2019",
    labFormat = labelFormat(prefix = ""),
    opacity = 1
  ) %>%
  addEasyButton(easyButton(
    states = list(
      easyButtonState(
        stateName="unfrozen-markers",
        icon="ion-toggle",
        title="Get Bounding box",
        onClick = JS("
                     function(btn, map) {
                        alert(
                         map.getBounds().getWest() + ', ' + map.getBounds().getEast() + ', ' + map.getBounds().getSouth() + ', ' + map.getBounds().getNorth() 
                        );
                        
                     }")
      )
    )
  )
)

```

## Map 2009 yearly average county negative equity rate

```{r}

# Filter for 2009
underwater_county_year_2009 <- underwater_county_year %>%
  dplyr::select(state_code, fips_code, state_name, county_name, y2009) %>%
  filter(y2009 >= 10)

# Join zip code coordinates to negative equity by zipcode
underwater_county_year_2009 <- geo_join(counties, underwater_county_year_2009, 'GEOID', 'fips_code', 
how = "inner")  

# Color Scheme
binpal <- colorBin("plasma", underwater_county_year_2009$y2009, 5, pretty = FALSE)

# Draw map
leaflet(underwater_county_year_2009) %>%
   addProviderTiles(providers$CartoDB.Positron) %>%
   #addProviderTiles(providers$Wikimedia) %>%
   addPolygons(fillColor = ~binpal(y2009), weight = 1, smoothFactor = 0.5, opacity = 0.1, fillOpacity = 0.5, color="black", popup = popupTable(underwater_county_year_2009)) %>%
   setView(-95, 39.335359608681216, 4) %>%
   addHomeButton(extent(-89.40673828125, -80.61767578125001, 31.23159167205059, 35.755428369259626), "GA") %>%
   addHomeButton(extent(-78.93676757812501, -74.54223632812501, 37.74031329210266, 39.854937988531276), "MD") %>%
   addHomeButton(extent(-76.85485839843751, -72.46032714843751, 38.90385833966778, 40.9840449469281), "NJ") %>%
   addHomeButton(extent(-75.88256835937501, -67.09350585937501, 40.44694705960048, 44.449467536006935), "NE") %>%
   addHomeButton(extent(-90.4888916015625, -86.0943603515625, 40.65147128144057, 42.67839711889055), "IL") %>%
   addHomeButton(extent(-84.79797363281251, -80.40344238281251, 40.91766362458114, 42.93631775765237), "MI") %>%
   addHomeButton(extent(-109.24804687500001, -100.458984375, 37.474858084971046, 41.65649719441145), "CO") %>%
   addHomeButton(extent(-136.45019531250003, -101.29394531250001, 28.38173504322311, 45.583289756006316), "CA") %>%
   addHomeButton(extent(-131.74804687500003, -61.43554687500001, 18.812717856407776, 52.908902047770255 ), "U.S.") %>%
   addLogo("https://jeroenooms.github.io/images/banana.gif",
        position = "bottomleft",
        offset.x = 5,
        offset.y = 40,
        width = 100,
        height = 100) %>%
   addLegend("bottomleft", 
             pal = binpal, 
             values = underwater_county_year_2009$y2009,
    title = "% Homes Negative Equity 2009",
    labFormat = labelFormat(prefix = ""),
    opacity = 1
  ) %>%
  addEasyButton(easyButton(
    states = list(
      easyButtonState(
        stateName="unfrozen-markers",
        icon="ion-toggle",
        title="Get Bounding box",
        onClick = JS("
                     function(btn, map) {
                        alert(
                         map.getBounds().getWest() + ', ' + map.getBounds().getEast() + ', ' + map.getBounds().getSouth() + ', ' + map.getBounds().getNorth() 
                        );
                        
                     }")
      )
    )
  )
)

```


## Explaining the differences
In general, amongst the highest negative equity zips, there's a general trend.  The higher the rate of af american and hispanics, the higher the rate.

```{r}

### Need to add in home price appreciation and mortgage origination as percentage of application
# Pull in NYT classification of urban rural

underwater_zips_2019_join <- underwater_zips_2019 %>%
  inner_join(acs_data_zcta, by=c("zip_code" = "geoid")) %>%
  inner_join(Zip_ZHVI_summary_current_month, by=c("zip_code" = "RegionName")) %>% 
  #%>%
  #filter(!is.na(percent_negative_equity),
   #      pct_white != "NaN",
   #      !is.na(median_household_income)) %>%
  filter(percent_negative_equity >=5) 
 
corr_table_zcta <- underwater_zips_2019_join %>%
  dplyr::select(-matches("code|name|Date|Region|State|Metro|County|City|Month|Quarter|Last|Time")) %>%
  dplyr::select(matches("percent|pct|year")) %>%
  correlate() %>%
  dplyr::select(rowname, percent_negative_equity)

print(corr_table_zcta)
#https://www.dataquest.io/blog/statistical-learning-for-predictive-modeling-r/
#install.packages("moderndive")

score_model <- lm(percent_negative_equity ~ pct_white+pct_black+pct_hispanic+PctFallFromPeak, data = underwater_zips_2019_join)
get_regression_table(score_model) %>% datatable()
get_regression_summaries(score_model) %>% datatable()

ggplot(underwater_zips_2019_join) +
  geom_point(aes(percent_negative_equity, pct_white)) +
  labs(x="Percent Negative Equity", y="Percent White", title="", caption = "") +
  geom_smooth(aes(percent_negative_equity, pct_white), method = "lm", se = FALSE) 

ggplot(underwater_zips_2019_join) +
  geom_point(aes(percent_negative_equity, pct_black)) +
  labs(x="Percent Negative Equity", y="Percent Black", title="", caption = "") +
  geom_smooth(aes(percent_negative_equity, pct_black), method = "lm", se = FALSE) 

ggplot(underwater_zips_2019_join) +
  geom_point(aes(percent_negative_equity, pct_hispanic)) +
  labs(x="Percent Negative Equity", y="Percent Hispanic", title="", caption = "") +
  geom_smooth(aes(percent_negative_equity, pct_hispanic), method = "lm", se = FALSE) 



```

At the county level, the effects are a bit more blunted. 




```{r}

# Join county level data by year with census data
underwater_county_year_join <- underwater_county_year %>%
  inner_join(acs_data_county, by=c("fips_code"="geoid")) %>%
  ungroup() %>%
  inner_join(county_summary_forecast, by=c("state" = "State", "county" = "RegionName")) %>%
  inner_join(rural_urban, by=c("fips_code" = "FIPS")) %>%
  mutate(housing_affordability = Zhvi/median_household_income) #%>%
  #filter(Population_2010 > 100000) %>%
  #filter(y2019 >=1)



#years <- c("y2018","y2017","y2016","y2015","y2014","y2013","y2012","y2011","y2010","y2009")

#for (year in years) {

underwater_county_year_join %>%
  dplyr::select(-matches("code|state|name|county|date|region|Metro|County|City|Month|Quarter|Last|Time|Description")) %>%
  #dplyr::select(matches("pct|median")) %>%
  #filter(!is.na(deparse(substitute(year)))) %>%
  correlate() %>%
  #dplyr::select(rowname, year) %>%
  filter(str_detect(rowname, "^y")) %>% 
  datatable(
    extensions = 'FixedColumns',
    options = list(
    pageLength = 50,
    dom = 't',
    scrollX = TRUE,
    fixedColumns = list(leftColumns = 2)
  )
  )

underwater_county_year_join <- underwater_county_year_join %>%
  dplyr::select(-matches("code|state|name|county|date|region|Metro|County|City|Month|Quarter|Last|Time|Description")) 

#install.packages("moderndive")

score_model <- lm(y2019~ ., subset(underwater_county_year_join, select=c( -y2009, -y2010,-y2011,-y2012,-y2013,-y2014,-y2015,-y2016,-y2017,-y2018)))
get_regression_table(score_model) %>% datatable()
get_regression_summaries(score_model) %>% datatable()

# Ratio of home prices to earnings by county? That's a feature to engineer and test
# Findings -- look at relationship between ZHVI and year.  It used to be much more strongly related to ZHVI in the past than it is now! Suggests it's no longer a function of housing prices.  Something else more important at work here. 
# Tends to be a metro area problem.....
# What would happen if we just filtered non rural


```


### Examine Change Over Time

```{r}

underwater_county_year_ranks <- underwater_county_year %>%
  filter(y2019 >= 1) %>%
  ungroup() %>%
  #na.omit() %>%
  mutate_at(vars(starts_with("y")), funs(round(percent_rank(.)*100,0))) %>% 
  rename_at(vars(starts_with("y")), function(x) paste0(x,"_rank"))

underwater_county_year_ranks <- underwater_county_year_ranks %>%
  left_join(underwater_county_year, by=c("state_code", "fips_code", "state_name", "county_name")) %>%
  left_join(acs_data_county, by=c("fips_code" = "geoid"))
  
print(underwater_county_year_ranks)



```

## Zip Code Data
CoreLogic data. Percent of homes with negative equity for all U.S. ZipCodes. Think data is Q1 2019, but not sure. 
```{r0}
# Load and clean data by fixing zip codes to add a leading zero for northeastern zip codes
underwater <- read_xlsx("../data/input_data/corelogic_underwater_homes.xlsx") %>%
  clean_names() %>%
  mutate(zip_code = clean.zipcodes(zip_code)) %>%
  mutate(pct_homes_negative_equity = round(share_of_homes_in_negative_equity*100, 2)) %>%
  select(-share_of_homes_in_negative_equity)

write_csv(underwater, "../data/output_data/underwater_cleaned_z.csv")

# Check for dup zip codes 

underwater %>%
  group_by(zip_code) %>%
  summarise(count = n()) %>%
  arrange(desc(count))

# Load ZCTA geography data 
acs_variable <- load_variables(2017, "acs5", cache = TRUE)

# Define census api key
census_api_key("549950d36c22ff16455fe196bbbd01d63cfbe6cf")

# Get zip code geography data with total population zcta 2017
acs_zcta_total_pop <- get_acs(geography = "county", variables = c("B06012_001"), geometry = TRUE, survey="acs5", year = 2017)

# Join it
zcta_underwater_geography <- acs_zcta_total_pop %>%
  inner_join(underwater, by = c("GEOID" = "zip_code"))

# anti join

zcta_anti <- underwater %>%
  anti_join(acs_zcta_total_pop, by = c("zip_code" = "GEOID"))

zcta_anti_x <- acs_zcta_total_pop %>%
  anti_join(underwater, by = c("GEOID" = "zip_code"), keepall=TRUE)

```

# Examine 




## Test Correlations

```{r}
# Get zip code geography data with total population zcta 2017
#acs_zcta_total_pop <- get_acs(geography = "zcta", variables = c("B06012_001"), geometry = TRUE, survey="acs5", year = 2017)

# Poverty B06012_002	Estimate!!Total!!Below 100 percent of the poverty level
acs_zcta_total_pov <- get_acs(geography = "zcta", variables = c("B06012_002"), geometry = TRUE, survey="acs5", year = 2017, summary_var = "B06012_001")

# Calculate MOE
acs_zcta_total_pov_x <- acs_zcta_total_pov %>%
  mutate(moe_percent = moe/estimate) %>%
  filter(moe_percent != "Inf") %>%
  filter(moe_percent < .2)

acs_zcta_total_pov_x <- acs_zcta_total_pov_x %>%
  left_join(underwater, by = c("GEOID" = "zip_code")) %>%
  mutate(pov_rate = estimate/summary_est)

  

```

## Examine the Data

High concentrations of negative equity in Connecticut, New Jersey and Maryland.

```{r}

underwater %>%
  filter(!is.na(share_of_homes_in_negative_equity)) %>%
  group_by(state_name) %>%
  summarise(mean_pct_underwater = mean(share_of_homes_in_negative_equity)*100) %>%
  arrange(desc(mean_pct_underwater))

```



```{r}
# County level analysis

underwater_x <- underwater %>%
  left_join(zip_county_crosswalk, by=c("zip_code" = "zip")) %>%
  group_by(zip_code) %>%
  summarise(count=n()) %>%
  arrange(desc(count)) %>%
  filter(count > 1)

# Build tables of share of homes in negative equity


underwater_counties <- underwater %>%
  filter(!is.na(share_of_homes_in_negative_equity)) %>%
  group_by(state_name, county_name) %>%
  summarise(mean_pct_underwater = mean(share_of_homes_in_negative_equity)*100) %>%
  arrange(desc(mean_pct_underwater)) %>%
  mutate(NAME = paste0())

# Get Census Data by County
census_api_key("549950d36c22ff16455fe196bbbd01d63cfbe6cf")

acs_county_total_pop <- get_acs(geography = "county", variables = c("B06012_001"), geometry = FALSE)


```

```{r}

# Define census api key
census_api_key("549950d36c22ff16455fe196bbbd01d63cfbe6cf")

# Examine ACS Variables
acs_variable <- load_variables(2017, "acs5", cache = TRUE)
# B07001 GEOGRAPHICAL MOBILITY IN THE PAST YEAR
  # Mobility B07001_017	Estimate!!Total!!Same house 1 year ago
#  B06012_002	 POVERTY STATUS IN THE PAST 12 MONTHS
  # Total Population B06012_001	Estimate!!Total
  # Poverty B06012_002	Estimate!!Total!!Below 100 percent of the poverty level
  # Poverty B06012_003	Estimate!!Total!!100 to 149 percent of the poverty level
  # Poverty B06012_004	Estimate!!Total!!At or above 150 percent of the poverty level

census_api_key("549950d36c22ff16455fe196bbbd01d63cfbe6cf")

acs_county_total_pop <- get_acs(geography = "county", variables = c("B06012_001"), geometry = FALSE)



acs_zip <- get_acs(geography = "zcta", variables = c("B06012_001"), geometry = FALSE)


acs <- acs %>%
  left_join(acs_variable, by = c("variable" = "name"))

head(orange)

orange %>%
  ggplot(aes(fill = estimate)) + 
  geom_sf(color = NA) + 
  coord_sf(crs = 26911) + 
  scale_fill_viridis_c(option = "magma") 


zip_county_crosswalk <- read_xlsx("../data/input_data/ZIP_COUNTY_092019.xlsx") %>%
  select(zip, county)

```
